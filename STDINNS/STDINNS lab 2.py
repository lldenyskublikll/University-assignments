# -*- coding: utf-8 -*-
"""STDINNS lab 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19bheNnQJtugvK0zb-kakbqwT-8FkqO9I
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense, Concatenate, SimpleRNN, Reshape
from keras import Input, Model
from sklearn.model_selection import train_test_split

"""#Functions visualisation part"""

# Generating x values for functions with step of 0.1 in range of 10
# Simple set with 0.1 step (good for 2d plots, bad for 3d (in our case))
# x_values = np.arange(-5, 5 , 0.1)

# This set is for more good-looking 3d projection
x_values = np.arange(0, 10 , 0.01).reshape(-1, 1)

# y = 0.2 * sin(3x) * x^2
y_values = 0.2 * np.sin(3 * x_values) * (x_values**2)

# z = sin|x| * sin(x + y)
z_values = np.sin(np.abs(x_values)) * np.sin(x_values + y_values)

# Combine x and y sets in 2d array
xy_values = np.concatenate((x_values, y_values), axis=1)

# funcs for testing and 3d plotting
def y_func(x):
    return 0.2 * np.sin(3 * x) * (x**2)

def z_func(x, y):
    return np.sin(np.abs(x)) * np.sin(x + y)

# y func visualisation
plt.plot(x_values, y_values, label="y = 0.2 * sin(3x) * x^2")
plt.xlabel("x")
plt.ylabel("y")
plt.grid(True)
plt.title("Graph of the y function (y = 0.2 * sin(3x) * x^2)")
plt.show()

# z func visualisation (2d)
plt.plot(x_values, z_values, label="z = sin|x| * sin(x + y)")
plt.xlabel("x")
plt.ylabel("z")
plt.grid(True)
plt.title("Graph of the z function (z = sin|x| * sin(x + y))")
plt.show()

'''
plt.plot(y_values, z_values, label="z = sin|x| * sin(x + y)")
plt.xlabel("y")
plt.ylabel("z")
plt.legend()
plt.grid(True)
plt.title("Graph of the y function (z = sin|x| * sin(x + y))")
plt.show()
'''

# z func visualisation (3d)
X, Y = np.meshgrid(x_values, y_values)
Z = z_func(X, Y)

fig = plt.figure()
ax = fig.add_subplot(1, 2, 2, projection = "3d")
ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none') # rstride=1, cstride=1, (don't need them)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')
plt.title("Graph of the z function (z = sin|x| * sin(x + y))")
plt.show()

"""#Neural networks part

Functions for model testing
"""

# Train and test sets
x_train_set, x_test_set, y_train_set, y_test_set = train_test_split(xy_values, z_values, test_size=0.2, random_state=42)

# Function for model testing
def Test_chosen_model(model, title_number, layers, neurons):
    # Show model info
    model.summary()

    # Compiling model
    model.compile(optimizer='adam', loss='mean_squared_error')

    # Training model
    history = model.fit(x_train_set, y_train_set, epochs=1000, batch_size=32, verbose=1,validation_data=(x_test_set, y_test_set))

    # Prediction
    z_predictions = model.predict(xy_values)

    # Plots for results
    if title_number == 1:
        title_text = f"Feed-Forward Backdrop NN ({layers} layer(s), {neurons} neuron(s))"
    if title_number == 2:
        title_text = f"Cascade-Forward Backdrop NN ({layers} layer(s), {neurons} neuron(s))"
    if title_number == 3:
        title_text = f"Elman Backdrop NN ({layers} layer(s), {neurons} neuron(s))"

    #    Plotting actual func data and predicted data
    plt.plot(x_values, z_values, label="Actual data")
    plt.plot(x_values, z_predictions, label="Predicted data")
    plt.xlabel("x")
    plt.ylabel("z")
    plt.grid(True)
    plt.title(f"Training results for {title_text}")
    plt.legend()
    plt.show()

    #    Plotting loss over epochs results
    plt.plot(history.history['loss'], label="Training loss over epochs")
    #plt.plot(history.history['val_loss'], label="Validation loss over epochs")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f'Loss over epochs (Performance:{"%.10g"%(history.history["loss"][-1])})')
    plt.legend()
    plt.show()

'''
# Neuron number for each NN variant
ff_1 = 10
ff_2 = 20
cf_1 = 20
cf_2 = 10
elm_1 = 15
elm_2 = 5
'''

"""Feedforward backdrop

"""

# Sequential FFB model
def FeedForwardBackdropModel(layers, neurons):
    # Sequential model creation
    model = Sequential(name = f'Feed_forward_NN_{layers}_layers_{neurons}_neurons')

    # Adding input layer and layer #1
    model.add(Dense(neurons, activation = 'relu', input_shape=(2,)))

    # Adding additional layers if it's needed
    for i in range(layers-1):
        model.add(Dense(neurons, activation = 'relu'))

    # Adding output layer
    model.add(Dense(1, name = "output"))

    return model

#   1 layer, 10 neurons
feed_forward_1 = FeedForwardBackdropModel(1, 10)
Test_chosen_model(feed_forward_1, 1, 1, 10)

#   1 layer, 20 neurons
feed_forward_2 = FeedForwardBackdropModel(1, 20)
Test_chosen_model(feed_forward_2, 1, 1, 20)

"""Elman backdrop"""

# Sequential EB model
def ElmanBackdropModel(layers, neurons):
    # Sequential model creation
    model = Sequential(name = f'Elman_NN_{layers}_layers_{neurons}_neurons')

    # Adding input reshape layer
    model.add(Reshape((1, 2, ), input_shape = (2,), name = "input_reshape"))

    # Adding first SimpleRNN layer and input layer
    model.add(SimpleRNN(neurons, return_sequences=True, input_shape=(2,), activation = 'relu'))

    # Adding additional layers if it's needed
    for i in range(layers-1):
        model.add(SimpleRNN(neurons, return_sequences=True, activation = 'relu'))

    # Adding output reshape layer
    model.add(Reshape((neurons, ), name = "output_reshape"))

    # Adding output layer
    model.add(Dense(1, name = "output"))

    return model

#   1 layer, 15 neurons
elman_backdrop_1 = ElmanBackdropModel(1, 15)
Test_chosen_model(elman_backdrop_1, 3, 1, 15)

#   3 layers, 5 neurons per each
elman_backdrop_2 = ElmanBackdropModel(3, 5)
Test_chosen_model(elman_backdrop_2, 3, 3, 5)

"""Cascade-forward backdrop"""

# Sequential CFB model
def CascadeForwardBackdropModel(layers, neurons):
    # Creating input layer
    inputLayer = Input(shape=(2,), name='input')

    # Creating first layer for NN
    current = Dense(neurons, activation='relu', input_shape=(2,))(inputLayer)

    # Creating combined layers which consist of previous and current layers
    for i in range(layers-1):
        concatenatedLayer = Concatenate()([inputLayer, current])
        # Dynamic calculation of input data for Dense layer
        current = Dense(neurons, activation='relu', input_shape=(2,))(concatenatedLayer)

    # Creating output layer
    outputLayer = Dense(1, name = 'output')(current)

    # Create model with all layers created above
    model = Model(inputs=inputLayer, outputs=outputLayer, name = f'Cascade_forward_NN_{layers}_layers_{neurons}_neurons')
    return model

#   1 layer, 20 neurons
cascade_forward_1 = CascadeForwardBackdropModel(1, 20)
Test_chosen_model(cascade_forward_1, 2, 1, 20)

#   2 layers, 10 neurons per each
cascade_forward_2 = CascadeForwardBackdropModel(2, 10)
Test_chosen_model(cascade_forward_2, 2, 2, 10)