# -*- coding: utf-8 -*-
"""STDINNS lab 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ghvDatxiLaGV_MywdaVjJWRpRIzCyHB7
"""

#======================================================================================
import random as rnd
import numpy as np
import matplotlib.pyplot as plt
#======================================================================================
import tensorflow as tf
import tensorflow_datasets as tfds
#======================================================================================
from keras import utils, callbacks
from keras.optimizers import Adam
from keras.models import Sequential
from keras.layers import Conv2D, BatchNormalization, MaxPool2D, Flatten, Dense, Dropout
from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator
#======================================================================================

"""#Loading dataset"""

# Loading dataset
lab_dataset, info = tfds.load('imagenette/160px-v2', with_info = True, as_supervised = True)

# Separating it in train and test parts
ds_train_part, ds_test_part = lab_dataset['train'], lab_dataset['validation']

# Getting info about train and test datasets sizes
train_ds_size = info.splits['train'].num_examples
test_ds_size = info.splits['validation'].num_examples

# Displaying datasets size and shape information
print(f"Datasets size:\n-- Train ==> {train_ds_size}\n-- Test ==> {test_ds_size}")

# Loading classes
class_number, image_labels = info.features['label'].num_classes, info.features['label'].int2str
label_names = [image_labels(id) for id in range(class_number)]

# Creating array with classes actual names
actual_names = ['Fish',           # Class #0
                'Dog',            # Class #1
                'Casette player', # Class #2
                'Chainsaw',       # Class #3
                'Church',         # Class #4
                'French horn',    # Class #5
                'Garbage truck',  # Class #6
                'Gas pump',       # Class #7
                'Golf ball',      # Class #8
                'Skydiver']       # Class #9

# Displaying these classes
print(f"Dataset has {class_number} classes in total:")
for class_id, label in enumerate(label_names):
    print(f'Class #{class_id} ==> {label}({actual_names[class_id]})')

# Creating func that shows images from splitted dataset
def show_images(dtst, size, act_name = actual_names, img_labels = image_labels):
    plt.figure(figsize = (15, 15))
    for i, (image, label) in enumerate(dtst.take(size)):
        plt.subplot(5, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(image)
        plt.xlabel(f"Class: #{label.numpy()}, {img_labels(label.numpy())}\n({act_name[label.numpy()]})")
    plt.show

# Viewing images from train dataset
show_images(ds_train_part, 25)

# Viewing images from test dataset
show_images(ds_test_part, 25)

"""#Preparing training/test data for model"""

# Preprocess image
def preprocess_image(img, lbl):
    img = tf.image.per_image_standardization(img)
    img = tf.image.resize(img, (227, 227))
    img = tf.clip_by_value(img, 0.0, 1.0)
    return img, lbl

# Adjusting datasets
ds_train_part_proc = ds_train_part.map(preprocess_image)
ds_test_part_proc = ds_test_part.map(preprocess_image)

# Viewing preprocessed images from train dataset
show_images(ds_train_part_proc, 25)

# Viewing preprocessed images from test dataset
show_images(ds_test_part_proc, 25)

"""# Creating AlexNet NN model"""

# Creating AlexNet NN model
def Create_AlexNet_NN_Model():
    model = Sequential([
        # Layer #1
        Conv2D(filters = 96, input_shape = (227, 227, 3), kernel_size = (11, 11), strides = (4, 4), padding = 'valid', activation = 'relu'),
        BatchNormalization(),
        MaxPool2D(pool_size = (3, 3), strides = (2, 2)),

        # Layer #2
        Conv2D(filters = 256, kernel_size = (5, 5), strides = (1, 1), padding = 'same', activation = 'relu'),
        BatchNormalization(),
        MaxPool2D(pool_size = (3, 3), strides = (2, 2)),

        # Layer #3
        Conv2D(filters = 384, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu'),
        BatchNormalization(),

        # Layer #4
        Conv2D(filters = 384, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu'),
        BatchNormalization(),

        # Layer #5
        Conv2D(filters = 256, kernel_size = (3, 3), strides = (1, 1), padding = 'same', activation = 'relu'),
        BatchNormalization(),
        MaxPool2D(pool_size = (3, 3), strides = (2, 2)),
        Flatten(),

        # Layer #6
        Dense(4096, activation = 'relu'),
        Dropout(0.5),

        # Layer #7
        Dense(4096, activation = 'relu'),
        Dropout(0.5),

        # Layer #8
        Dense(10, activation = 'softmax')
    ], name = 'AlexNet_neural_network')
    return model

# Creating new AlexNet NN model and compiling it
alex_net_model = Create_AlexNet_NN_Model()
alex_net_model.compile(optimizer = 'adam',
              loss = 'sparse_categorical_crossentropy',
              metrics = ['accuracy'])
alex_net_model.summary()

"""#Model training"""

# training model
history = alex_net_model.fit(ds_train_part_proc.batch(64), epochs = 40, validation_data = ds_test_part_proc.batch(64))

"""# Training and Test(Validation) Loss and Accuracy"""

# Plotting "Loss over epochs" graph
plt.plot(history.history['loss'], label = 'Train loss')
plt.plot(history.history['val_loss'], label = 'Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('"Loss over epochs" graph')
plt.legend()
plt.show()

# Plotting "Accuracy over epoch" graph
plt.plot(history.history['accuracy'], label = 'Train accuracy')
plt.plot(history.history['val_accuracy'], label = 'Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('"Accuracy over epochs" graph')
plt.ylim([0, 1])
plt.legend()
plt.show()

# Getting info about  final loss and accuracy for both train and test
train_loss, train_accuracy = alex_net_model.evaluate(ds_train_part_proc.batch(64))
test_loss, test_accuracy = alex_net_model.evaluate(ds_test_part_proc.batch(64))

print(f'Train loss: {train_loss}')
print(f'Train accuracy: {train_accuracy}')

print(f'Test loss: {test_loss}')
print(f'Test accuracy: {test_accuracy}')

# Saving trained model
alex_net_model.save('alexnet_trained_model_imagenette_v2.keras')

"""# Viewing predictions on test dataset images"""

# Viewing prediction of first image from test dataset
#test_predictions = model.predict(ds_test_part)

def image_prediction_grid(dtst, size, act_name = actual_names, img_labels = image_labels):
    plt.figure(figsize = (21, 21))
    for i, (image, label) in enumerate(dtst.take(size)):
        plt.subplot(5, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(image)
        real_label = label.numpy()
        preproc_img = preprocess_image(image, label)
        image_for_pred = preproc_img[0]
        image_for_pred = tf.expand_dims(image_for_pred, axis = 0)
        prediction = alex_net_model.predict(image_for_pred)
        pred_label = np.argmax(prediction)

        plt.xlabel(f"Real: #{real_label}, {img_labels(real_label)} ({act_name[real_label]})\n" +
                   f"Predicted: #{pred_label}, {img_labels(pred_label)} ({act_name[pred_label]})",
                   color = 'green' if pred_label == real_label else 'red')
    plt.show

image_prediction_grid(ds_test_part, 25)

# unused plot func elements
'''
    plt.figure(figsize = (15, 15))
    for i in range(size):
        img_id = np.random.randint(len(dtst))
        image, label = dtst[img_id]
        label_index = np.argmax(label)
        plt.subplot(5, 5, i + 1)
        plt.xticks([])
        plt.yticks([])
        plt.grid(False)
        plt.imshow(image[0])
        plt.xlabel(f"Class: #{label_index}, {image_labels(label_index)}\n({act_name[label_index]})")
'''
'''
    plt.figure(figsize = (20, 20))
    for i in range(25):
        img_id = np.random.randint(len(dtst))
        image, image_label = dtst[img_id]
        prediction = (model.predict(dtst))[img_id]
        predict_label = np.argmax(prediction)
        real_label = np.argmax(image_label)
        plt.subplot(5, 5, i + 1)
        plt.grid(False)
        plt.xticks([])
        plt.yticks([])
        plt.imshow(image[0])
        plt.xlabel(f"Real: class #{real_label}, {img_labels(real_label)} ({act_name[real_label]})\n" +
                   f"Predicted: class #{predict_label}, {img_labels(predict_label)} ({act_name[predict_label]})",
                   color = 'green' if predict_label == real_label else 'red')
'''

# unused dataset and image preprocess elements
'''
# Preparing training and test data for model
# (from train and test parts of imagenette dataset)
x_train = list(map(lambda img: img[0], ds_train_part))
y_train = list(map(lambda img: img[1], ds_train_part))

x_test = list(map(lambda img: img[0], ds_test_part))
y_test = list(map(lambda img: img[1], ds_test_part))

# Switching y train/test values to cathegorical
y_train = utils.to_categorical(y_train, image_size)
y_test = utils.to_categorical(y_test, image_size)
'''
'''
# Creating data generators for train and test datasets
# (we'll use them for image px scaling)
train_data_generator = ImageDataGenerator(rescale = 1./255)
test_data_generator = ImageDataGenerator(rescale = 1./255)

ds_train_part = NumpyArrayIterator(x = np.array(x_train), y = np.array(y_train),
                                   image_data_generator = train_data_generator,
                                   batch_size = 64)
ds_test_part = NumpyArrayIterator(x = np.array(x_test), y = np.array(y_test),
                                   image_data_generator = test_data_generator,
                                   batch_size = 64)
'''